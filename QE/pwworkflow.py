# %%
from aiida.orm import QueryBuilder, Node
import aiida.orm as orm
from aiida.engine import run
from aiida.orm.groups import Group
from aiida.orm import Dict, KpointsData, StructureData, load_code, load_group, load_node, load_computer
from aiida import engine
from aiida.orm.nodes.process.calculation.calcjob import CalcJobNode
from aiida.common.links import LinkType

from aiida_quantumespresso.workflows.pw.base import PwBaseWorkChain

from aiida_quantumespresso.workflows.pw.base import PseudoDojoFamily

from aiida.engine import submit

from aiida.tools.visualization.graph import Graph


from aiida.engine import calcfunction, workfunction

from aiida import load_profile, get_profile
if get_profile() == None:
    load_profile()




## try workflow of pw.x

from aiida.plugins.factories import CalculationFactory, WorkflowFactory

# PwBaseWorkChain = WorkflowFactory('quantumespresso.pw.base')
PwBandsWorkChain = WorkflowFactory('quantumespresso.pw.bands')


### some constants
list_bandsx_inputs_file  = ['./aiida.out', 
                            './out/aiida.wfc1',
                            './out/aiida.wfc2',
                            './out/aiida.xml',
                            './out/aiida.save/charge-density.dat',
                            './out/aiida.save/data-file-schema.xml',
                            ]


localcomputer = load_computer('localhost_direct')
localcomputer_workdir = load_computer('localhost_direct').get_workdir().__str__()




def prepare_pw_calcjob(atoms_uuid, 
                              code_name, 
                              input_para: dict, 
                              pseudo_family_name: str = 'PseudoDojo/0.4/PBE/FR/standard/upf',
                              k_points_mesh = [10, 10, 1],
                              k_points_offset = [0, 0, 0],
                              n_cond_bands : int = 6, ## in pseudopotential's we can see how many electron othsize of PP. They are than valence band, and conduction bands are not occupied.
                              withmpi : bool = False, ## MPI support : parallisation
                              num_node : int = 1 , ## node to use
                              memory_per_node : int = 2048, ## memory in MB per node
                              queue_name : str = 'cpu-ondemand', 
                              qos :str = 'ondemand-short', ## othewise if you want >1hour per node, use 'cpu-ondemand-long'
                              time : str = '00:30:00',
                              prepend_text : str = 'module load bioinfo-ifb openmpi/4.0.4 gcc/11.2.0\nexport NEWVAR=1', # 
                              Folder_node_scf = None ## for nscf calculation, you need to provide the scf calculation folder node from output of scf calcjob
                              
                              ):
    """
    Prepare inputs link to the 2D crystal for PwBaseWorkChain. Except the input_para containing para independent to structure who is generated by config_input_para_pw function.
    

    Args :
    ------
        code_name : str : **@COMPUTER composited by ** in **.x and COMPUTER at the host where you compile the **.x. E.g. pw@localhost 
                look for the COMPUTER with 'verdi computer list' in your terminal.

        Folder_node_scf : aiida.orm.FolderData : The output folder node of the scf calculation. Only needed when you want to do nscf calculation. 
                        If you set input_para['CONTROL']['calculation'] = 'nscf', you need to provide this argument. If you set it to 'scf', this argument will be ignored.
        atoms_uuid : str : uuid of the structure you want to calculate. It should be in your database.
        input_para : dict : input parameters independent to the structure. You can generate it with config_input_para_pw function.
        pseudo_family_name : str : name of the pseudopotential aiida group you want to use. It should be in your aiida database. Check with 'verdi group list' in your terminal.
        k_points_mesh : list : k-points mesh in reciprocal space. Default is [10, 10, 1] for 2D materials. 
                            if you lanche nscf calculation, you may want to set it denser (than the scf calculation) to get better band structure.
        k_points_offset : list : k-points offset in reciprocal space. Default is [0, 0, 0].
        n_cond_bands : int : number of conduction bands to be calculated. (how many bands above the valence electron in your pseudopotential file).

        withmpi : bool : whether to use MPI for parallelization. Default is False.
        num_node : int : number of nodes (cpu or gpu) to use for the calculation. Default is 1.

        ###Arguments below : Some option link to submission of file *.sh to a HPC cluster. See the cluster's documentation. Below is for ISDM-meso cluster at montpellier.
        queue_name : str : name of the queue to submit the calculation. Default is 'cpu-ondemand' for ISDM cluster.
        qos : str : quality of service. Default is 'ondemand-short' for ISDM cluster. If you want to run longer than 1 hour, use 'cpu-ondemand-long'.
        time : str : time limit for the calculation in format 'HH:MM:SS'. Default is '00:30:00'. Only needed when qos is 'ondemand-short' in ISDM cluster.
        prepend_text : str : text to prepend to the submission script. You can use it to load modules or set environment variables. Default is set to load in ISDM-meso cluster.
        
        
        

    Returns:
    ------
        builder : builer from the code get_builder() function. You can run it with aiida.engine.run or submit function.
        None : if there is an error in loading the structure or pseudopotential family. For example, if pseudopotential family doesn't contain all elements in the structure.
    
    """
    
    atoms = load_node(atoms_uuid)
    if not isinstance(atoms, StructureData):
        print(f"Error skiped : The node with UUID {atoms_uuid} is not a StructureData node. Skip this error Return None")
        return None
    
    

    code = load_code(code_name)
    pseudo_struc = None ## just initial it before try-except
    #### Get structure and their pseudopotentials & cutoffs recommended by the file itself(Like upf in PseudoDojo)
    try: ### Pseudopotential family can't contain all elements. Some elements are not calculated.
        
        pseudo_family = orm.load_group(pseudo_family_name)
        pseudo_struc = pseudo_family.get_pseudos(structure=atoms)  ## return a dict of {symbol: pseudo}
    except Exception as e:
            print(f"Error loading pseudopotential family or getting pseudos/cutoffs: {e}")
            return None
    
    
    cutoffs = pseudo_family.get_recommended_cutoffs(structure=atoms, unit='Ry') ## return a tuple(wfc cutoff, rho cutoff) in eV. It's the max of all elements in structure.
    cutoffs_wfc, cutoffs_rho = cutoffs
    total_valence_electrons = 0
    dict_num_each_element = atoms.get_composition() ## dict of {symbol of element: number of atoms of this element}
    for key, value in pseudo_struc.items():
        valence_n = value.z_valence
        
        total_valence_electrons += valence_n * dict_num_each_element[key]
    nbnd = -1
    if input_para['SYSTEM']['lspinorb']:
        nbnd = total_valence_electrons + n_cond_bands ## even having N fold degeneracy, bands will be annotated in N diff index 
    else:
        nbnd = total_valence_electrons // 2 + n_cond_bands 
    input_para['SYSTEM']['nbnd'] = nbnd
    ## add cutoffs to input_para
    
    input_para['SYSTEM']['ecutwfc'] = cutoffs_wfc
    input_para['SYSTEM']['ecutrho'] = cutoffs_rho

   
    
    # Get builder from protocol

    # PwBaseWorkChain.get_builder_from_protocol
    # builder = PwBaseWorkChain.get_builder()
    code = load_code(code_name)
    builder = code.get_builder()
    
    # builder.pw.code = orm.load_code('pw@localhost')
    # builder.pw.code = code
    builder.structure = atoms
    builder.pseudos = pseudo_struc
    

    # Define K-point mesh in reciprocal space 
    kpoints = KpointsData()
    kpoints.set_kpoints_mesh(k_points_mesh, k_points_offset)
    builder.kpoints = kpoints
    
    if  input_para['CONTROL']['calculation'] == 'scf':
        # builder.parent_folder = None
        pass
    elif input_para['CONTROL']['calculation'] == 'nscf':
        builder.parent_folder = Folder_node_scf
    builder.parameters = Dict(input_para)
    # results = engine.run(builder)

    
    # host = code.computer.get_configuration()['gss_host']
    # if host == 'localhost': ## Out of this case remove


    ### check if the code is running on localhost or HPC cluster
    try :  
        code.computer.get_configuration()['gss_host'] ## if gss_host exist, it's a ssh host
    except :
        qos = None
        time = None
        queue_name = None


    ### some options link to the calcul sumbition file *.sh created automaticlly in the cluster.
    options = {'resources': {
                    'num_machines': num_node,
                },
                # 'max_wallclock_seconds': 1800,
                'withmpi': withmpi, ### MPI support (parallelaisation)
                'queue_name': queue_name,  ## you can specify the queue name of your HPC cluster
                'qos' : qos,
                'max_memory_kb': memory_per_node * 1024,  ## memory in kb per node
                
                # ## load necessary module in the cluster. This part is not link to aiida so it's needed to be writed your self.
                #  : 'module load bioinfo-ifb openmpi/4.0.4 gcc/11.2.0'
                #  ## you can add some extra commands in the submission file, like module load xxx
                
                'prepend_text' : prepend_text


                
                }
    if qos == 'ondemand-short':
        options['custom_scheduler_commands'] = f'#SBATCH --time={time}'    ## --time option in ISDM cluster. Normally it's the max time running wall
    elif qos == 'cpu-ondemand-long':
        # options['custom_scheduler_commands'] = 'module load bioinfo-ifb openmpi/4.0.4 gcc/11.2.0'
        pass
    elif qos is None:
        pass
    else: ## other qos in ISDM cluster is not allowed (for me)
        raise ValueError("qos should be 'cpu-ondemand-long' or 'ondemand-short' or 'None' (for localhost calculation)")
    
    builder.metadata.options = options ## --time option in ISDM cluster. Normally it's the max time running wall

    #### check if you did once scf calculation before with same parameter.

    

    return builder



def check_if_structure_processed_scf_wellfinished_same_para(atoms_uuid, input_para: dict):
    """
    if a structure have a scf calculation done, should not repeat it again.
        Args:
        ----------
            atoms_uuid : str
                UUID of the structure to check.
            input_para : dict
                Input parameters used in the calculation to compared.
    
    TODO: finishi this and intergrate it into prepare_pw_calcjob function to avoid repeat scf calculation.
    """
    
    g = Graph()
    struc_node = load_node(atoms_uuid)
    if not isinstance(struc_node, StructureData):
        raise ValueError(f"The node with UUID {atoms_uuid} is not a StructureData node.")
    g.add_node(struc_node)
    pks_out = g.add_outgoing(node=struc_node)
    is_same_calculation = False
    while not is_same_calculation:
        calcjob_node = load_node(pk)
        if isinstance(calcjob_node, orm.CalcJobNode):
            if calcjob_node.process_label == 'PwCalculation':
                pks_in_calc = g.add_incoming(node=calcjob_node)
                for pk in pks_in_calc:
                    pass




def config_input_para_pw(scf :bool = False,
                         lspinorb : bool = True,
                         occupations : str = 'fixed',
                         assume_isolated = '2D',
                         ibrav = 0,
                         ### End of system parameters
                         ### control parameters
                         etot_conv_thr =   1.0000000000e-05,
                         forc_conv_thr = 1.0000000000e-04,
                         max_seconds =   3.4200000000e+04,
                         
                         restart_mode = 'from_scratch',
                         verbosity = 'high', 
                         wf_collect = True, ## wave function collection
                         ### electron parameters
                         conv_thr =   1.0000000000e-10,
                         

                         ):
    """
    Configure the input dictionary for a PwBaseWorkChain run. This is basicly the input file of pw.x (except the structure and atoms specis linked info)


    Args:
    ----------
        scf : bool
        Whether to perform a self-consistent field calculation (True) or a non-self-consistent field calculation (False).
        Default is False.

    Returns:
    -------
        inputs : dict
        A dictionary containing the inputs for the PwBaseWorkChain.

    """

    scf_val = 'scf' if scf else 'nscf'
    noncolin = True if lspinorb else False
    
    

    inputs = {
        'SYSTEM': {
            'lspinorb' : lspinorb,
            'noncolin' : noncolin,
            'occupations' : occupations,
            'assume_isolated' : assume_isolated,
            'ibrav' : ibrav
            
        },
        'CONTROL': {
            'calculation': scf_val, ## 'scf' if scf else 'nscf'
            'etot_conv_thr' : etot_conv_thr,
            'forc_conv_thr' : forc_conv_thr,
            'max_seconds' : max_seconds,            
            'restart_mode' : restart_mode,
            'verbosity' : verbosity,
            'wf_collect' : wf_collect
            
        },
        'ELECTRONS' : {'conv_thr' : conv_thr},
        
        
    }
    return inputs



def workflow_scf_nscf(atoms_uuid, 
                    code_name, 
                    input_para: dict, 
                    pseudo_family_name: str = 'PseudoDojo/0.4/PBE/FR/standard/upf',
                    k_points_mesh = [10, 10, 1],
                    k_points_mesh_nscf = [24, 24, 1], ## this is for nscf calculation, normally denser than scf above
                    k_points_offset = [0, 0, 0],
                    n_cond_bands : int = 6, ## in pseudopotential's we can see how many electron othsize of PP. They are than valence band, and conduction bands are not occupied.
                    withmpi : bool = False, ## MPI support : parallisation
                    num_node : int = 1 , ## node to use
                    memory_per_node : int = 2048, ## memory in MB per node
                    queue_name : str = 'cpu-ondemand', 
                    qos :str = 'ondemand-short', ## othewise if you want >1hour per node, use 'cpu-ondemand-long'
                    time : str = '00:30:00',
                    if_scf_done_folderdata :str = None, ## if you already have a scf calculation done, you can provide the RemoteData node uuid here to skip the scf calculation step.
                    prepend_text : str = 'module load bioinfo-ifb openmpi/4.0.4 gcc/11.2.0\nexport NEWVAR=1', # 
                    # Folder_node_scf = None ## for nscf calculation, you need to provide the scf calculation folder node from output of scf calcjob):
                    ):
    """
    This workflow is the following steps:
    1, build a scf calcjob with prepare_pw_calcjob function and submit it
    2, from the sumbition above get the FolderData node of the scf calcjob, which contains the output path of scf calculation.
    3, reload the prepare_pw_calcjob with nscf option.
    4, submit the nscf calcjob.

    Args :
    -------
    Same as prepare_pw_calcjob function and config_input_para_pw function

    and additional argument :
        if_scf_done_folderdata : str : uuid of the FolderData node of a scf calculation already done. If you provide this argument, the workflow will skip the scf calculation step and directly do the nscf calculation.
    
    """
    if if_scf_done_folderdata is None:
        builder_scf = prepare_pw_calcjob(atoms_uuid, 
                                            code_name, 
                                            input_para, 
                                            pseudo_family_name=pseudo_family_name,
                                            k_points_mesh=k_points_mesh,
                                            k_points_offset=k_points_offset,
                                            n_cond_bands=n_cond_bands,
                                            withmpi=withmpi,
                                            num_node=num_node,
                                            memory_per_node=memory_per_node,
                                            queue_name=queue_name,
                                            qos=qos,
                                            time=time,
                                            prepend_text=prepend_text,
                                            Folder_node_scf=None
                                            )
        if builder_scf is not None:
            calcjob_node_scf = submit(builder_scf)
            scf_pk = calcjob_node_scf.pk
            print("SCF calcjob submitted successfully.")
            import time as tm
            ### from scf calcjob get the output folder node
            while not calcjob_node_scf.is_terminated:
                tm.sleep(3) ## check every 3s if the calcjob is finished
                calcjob_node_scf = load_node(scf_pk)
            
            # if calcjob_node_scf.is_failed:
            #     print("SCF calculation failed. Cannot proceed to NSCF calculation.")
            #     return None
            # elif calcjob_node_scf.is_finished_ok:
            try:
                print("SCF calculation finished successfully. Proceeding to NSCF calculation.")
                Folder_node_scf = calcjob_node_scf.outputs.remote_folder
                # Folder_node_scf = calcjob_node_scf.get_remote_workdir()
                input_para['CONTROL']['calculation'] = 'nscf'
            
                builder_nscf = prepare_pw_calcjob(atoms_uuid, 
                                                        code_name, 
                                                        input_para, 
                                                        pseudo_family_name=pseudo_family_name,
                                                        k_points_mesh=k_points_mesh_nscf, ## this is changed compared to scf.
                                                        k_points_offset=k_points_offset,
                                                        n_cond_bands=n_cond_bands,
                                                        withmpi=withmpi,
                                                        num_node=num_node,
                                                        memory_per_node=memory_per_node,
                                                        queue_name=queue_name,
                                                        qos=qos,
                                                        time=time,
                                                        prepend_text=prepend_text,
                                                        Folder_node_scf=Folder_node_scf
                                                        )
                if builder_nscf is not None:
                    calcjob_node_nscf = submit(builder_nscf)
                    print("NSCF calcjob sumbitted successfully.")
                    return calcjob_node_scf, calcjob_node_nscf
            except Exception as e:
                # print("SCF calculation did not finish properly. Cannot proceed to NSCF calculation.")
                print(f"SCF terminated but NSCF didnt do well: {e}")
                pass
                
    else:
        Folder_node_scf = load_node(if_scf_done_folderdata)
        input_para['CONTROL']['calculation'] = 'nscf'
        builder_nscf = prepare_pw_calcjob(atoms_uuid, 
                                                        code_name, 
                                                        input_para, 
                                                        pseudo_family_name=pseudo_family_name,
                                                        k_points_mesh=k_points_mesh_nscf, ## this is changed compared to scf.
                                                        k_points_offset=k_points_offset,
                                                        n_cond_bands=n_cond_bands,
                                                        withmpi=withmpi,
                                                        num_node=num_node,
                                                        memory_per_node=memory_per_node,
                                                        queue_name=queue_name,
                                                        qos=qos,
                                                        time=time,
                                                        prepend_text=prepend_text,
                                                        Folder_node_scf=Folder_node_scf
                                                        )
        if builder_nscf is not None:
            calcjob_node_nscf = submit(builder_nscf)
            print("NSCF calcjob sumbitted successfully.")
            return None, calcjob_node_nscf
        

from aiida.orm import load_node, FolderData, RemoteData
import tempfile, os, shutil

# @workfunction
def copy_calcjob_outputs_to_local_deprecated(calcjob_node):
    """
    Given a CalcJob PK, copy all FolderData and RemoteData outputs to new
    FolderData nodes in the current (local) AiiDA profile.
    Returns: {original output PK: transcalculation job node}

    Note:
    -----
        This function is undown. I dont really understand the TransferCalculation objet.
    """
    # calcjob_node = load_node(calcjob_node)
    remote_folder_node = calcjob_node.outputs.remote_folder
    if not isinstance(remote_folder_node, RemoteData):
        raise ValueError("The provided node does not have a RemoteData output.")
    
    remote_folder = remote_folder_node.get_remote_path()
    relative_remote_path = os.path.relpath(remote_folder, remote_folder_node.computer.get_workdir())
    computer = load_computer('localhost_direct')

    # get files' relative path to the remote_folder to copy.
    # list_paths = calcjob_node.get_retrieve_list()  ## get the list of files to retrieve

    # destinat_local_folder = load_computer(local_computer_name).get_workdir()
    # targer_path = os.path.join(destinat_local_folder, relative_remote_path)
    instructions_cont = {}
    instructions_cont['retrieve_files'] = True
    instructions_cont['symlink_files'] = [
       ('node_keyname','.', '.'),
    ]
    instructions_node = orm.Dict(dict=instructions_cont)
    


    ## transfer funciton
    transfer_builder = CalculationFactory('core.transfer').get_builder()
    transfer_builder.instructions = instructions_node
    transfer_builder.source_nodes = {'node_keyname': remote_folder_node}
    transfer_builder.metadata.computer = remote_folder_node.computer
    
    return transfer_builder






# @workfunction

def copy_calcjob_outputs_to_local(calcjob_node : CalcJobNode,
                                  localcomputer = localcomputer,
                                  list_files_paths = None, ## related file path to the remote_folder
                                  copy_all = False, ## if true, just copy all the out folder in the remote folder
                                  workdir = localcomputer_workdir
                                  ):
    # remote_folder_node = calcjob_node.outputs.remote_folder
    remote_folder_node = calcjob_node.outputs.remote_folder
    # creator = remote_folder_node.creator
    # # VÃ©rifier si c'est un CalcJobNode
    # calcjob_node = creator
    # if not isinstance(creator, CalcJobNode):
    #     raise ValueError("The provided node was not created by a CalcJobNode.")
    if not isinstance(remote_folder_node, RemoteData):
        raise ValueError("The provided is not a RemoteData.")
    

    remote_folder = remote_folder_node.get_remote_path().__str__()
    # uuid_remote_node = remote_folder_node.uuid
    uuid_calcjob = str(calcjob_node.uuid)
    # use scp to copy the folder to local computer
    # computer = load_computer('localhost_direct')
    destinat_local_folder = workdir + '/' + 'ISDM_results' + '/' + 'copied_from' + '_' + uuid_calcjob 
    if copy_all:
        
        
        os.makedirs(destinat_local_folder, exist_ok=True)
        
        scp_command = f"scp -r 'junliny@io-login.meso.umontpellier.fr':{remote_folder} {destinat_local_folder}"

        exit_code = os.system(scp_command)
        
        
    else :
    
        if list_files_paths is None:
            list_files_paths = calcjob_node.get_retrieve_list() ### note that the list doesnt containt all files and folder in the remote_folder. but sufficient (for bands.x).
        
        for file_path in list_files_paths:
            # if os.path.isabs(file_path):
            #     abs_file_path = file_path
            # else:
            #     abs_file_path = os.path.join(remote_folder, file_path)
            related_folder = os.path.dirname(file_path)
            folder_local_to_save = destinat_local_folder + '/' + related_folder
            os.makedirs(folder_local_to_save, exist_ok=True)
            scp_command = f"scp 'junliny@io-login.meso.umontpellier.fr':{remote_folder + '/' + file_path} {folder_local_to_save}"
            # rename_command = f"mv {destinat_local_folder}/{file_path} {destinat_local_folder}/{'copiedfrom' + uuid_remote_node}_{file_path}"
            os.system(scp_command)
            # os.system(rename_command)
    
    node_folder = RemoteData(f"{destinat_local_folder + '/' + remote_folder.split('/')[-1]}"  )
    node_folder.computer = localcomputer
    node_folder.label = f'local copy from remote calcjob {uuid_calcjob}'
    node_folder.store()


    # ## create link to input remote folder
    # node_folder.base.links.add_incoming(
    #     calcjob_node.outputs.remote_folder,                    # Node source
    #     link_type=LinkType.INPUT_WORK,        # Type of the link
    #     link_label=f'local_copy_from_{calcjob_node.pk}'           # Link label
    # )
    
    return node_folder


def find_nodes_by_calcjob_uuid(uuid_calcjob):
    """
    to find le node local copy from remote calcjob with uuid_calcjob
    """
    label_to_find = f'local copy from remote calcjob {uuid_calcjob}'
    qb = QueryBuilder()
    qb.append(Node, filters={"label": label_to_find})
    if qb.all().__len__() == 0:
        raise ValueError(f"No node found with label {label_to_find}")
    elif qb.all().__len__() > 1:
        raise ValueError(f"Multiple nodes found with label {label_to_find}, returning all of them.")
        
    return qb.all()[0][0]  # retourne la liste des nodes correspondants




def bands_calc_input():
    inputs = {'bands': {
  
            # 'filband' : 'spin',
            'lsym' : False,
            'lsigma(1)' : True,
            'lsigma(2)' : True,
            'lsigma(3)' : True,
            # 'plot_2D' : True,
            }}
    return inputs

def bands_calc_builder(inputs : dict = bands_calc_input(),
                       code_name : str = 'bands@localhost_direct',
                       parent_folder = None, ## where you did your nscf calculation, or scf
                       queue_name : str = 'cpu-ondemand', 
                       qos :str = 'ondemand-short', ## othewise if you want >1hour per node, use 'cpu-ondemand-long'
                       time : str = '00:30:00',
                       prepend_text : str = 'module load bioinfo-ifb openmpi/4.0.4 gcc/11.2.0\nexport NEWVAR=1', # 
                       withmpi : bool = False, ## MPI support : parallisation
                        num_node : int = 1 , ## node to use
                        memory_per_node : int = 2048, ## memory in MB per node
                       ):
    code = load_code(code_name)
    builder = code.get_builder()

    builder.parameters = Dict(inputs)
    builder.parent_folder = parent_folder

    try :  
        code.computer.get_configuration()['gss_host'] ## if gss_host exist, it's a ssh host
    except :
        qos = None
        time = None
        queue_name = None

    ### some options link to the calcul sumbition file *.sh created automaticlly in the cluster.
    options = {'resources': {
                    'num_machines': num_node,
                },
                # 'max_wallclock_seconds': 1800,
                'withmpi': withmpi, ### MPI support (parallelaisation)
                'queue_name': queue_name,  ## you can specify the queue name of your HPC cluster
                'qos' : qos,
                'max_memory_kb': memory_per_node * 1024,  ## memory in kb per node
                
                # ## load necessary module in the cluster. This part is not link to aiida so it's needed to be writed your self.
                #  : 'module load bioinfo-ifb openmpi/4.0.4 gcc/11.2.0'
                #  ## you can add some extra commands in the submission file, like module load xxx
                
                'prepend_text' : prepend_text


                
                }
    if qos == 'ondemand-short':
        options['custom_scheduler_commands'] = f'#SBATCH --time={time}'    ## --time option in ISDM cluster. Normally it's the max time running wall
    elif qos == 'cpu-ondemand-long':
        # options['custom_scheduler_commands'] = 'module load bioinfo-ifb openmpi/4.0.4 gcc/11.2.0'
        pass
    elif qos is None:
        pass
    else: ## other qos in ISDM cluster is not allowed (for me)
        raise ValueError("qos should be 'cpu-ondemand-long' or 'ondemand-short' or 'None' (for localhost calculation)")
    
    builder.metadata.options = options ## --time option in ISDM cluster. Normally it's the max time running wall
    return builder














































# %% 
if __name__ == '__main__':
    ## Example usage
    ## 1. Prepare input parameters
    # %load_ext aiida
    # %aiida
    # structures = load_group("structure_2D").nodes

    input_para = config_input_para_pw(scf=True)
    
    ## 2. Prepare PwBaseWorkChain builder
    # atoms_uuids = [
    #                 # 'd25962ea-3c39-4e86-9333-2baf8f31a1a8',  # MoS2
    #             #    'f2c23dd6-7ef1-4c6b-9bc9-973e63905a0d', # GeTe
    #             # 'de5d09ce-bf2d-4ba9-87d7-f342b2a2a636' # As203 P6mm
    #                ]
    
    # atoms_uuids = load_node(519524).get_list() ## AB compounds
    atoms_uuids = [
                    # 'f2c23dd6-7ef1-4c6b-9bc9-973e63905a0d', ## GeTe
                    # 'de5d09ce-bf2d-4ba9-87d7-f342b2a2a636', ## As2O3 P6mm C6v
                    '9be800e5-41ac-4875-8746-e10b6995cb8b', ## AsSb P3m1 C3v
                    '481b517b-3fd8-4fad-8608-ce2846c61ed1', ##  PbTe P3m1 C3v
                    ]
    for uuid in atoms_uuids:
        Z_num_max = load_node(uuid).get_ase().get_atomic_numbers().max()
        if Z_num_max > 56 and Z_num_max < 72: ### just Lanthanides
            atoms_uuids.remove(uuid)
    
    
    code_name = 519349 # pw@ISDM_jy my pw code on ISDM-meso cluster
    
    calcjob_node_uuid = {}
# %% for a given list of structures, do scf and nscf calculation and copy the nscf output to local computer
    uuids_nscf_calcjob_dict = {}
    for uuids in atoms_uuids:
        atoms_uuid = uuids
        calcjob_scf, calcjob_nscf = workflow_scf_nscf(atoms_uuid, 
                                                    code_name, 
                                                    input_para, 
                                                    pseudo_family_name='PseudoDojo/0.4/PBE/FR/standard/upf',
                                                    k_points_mesh = [10, 10, 1],
                                                    k_points_mesh_nscf = [24, 24, 1],
                                                    k_points_offset = [0, 0, 0],
                                                    n_cond_bands = 6,
                                                    withmpi = True,
                                                    num_node = 1,
                                                    queue_name = 'cpu-ondemand',
                                                    qos ='ondemand-short',
                                                    time = '00:30:00',
                                                    prepend_text = 'module load bioinfo-ifb openmpi/4.0.4 gcc/11.2.0\nexport NEWVAR=1'
                                                    
                                                    )
        if calcjob_nscf is not None:
            uuids_nscf_calcjob_dict[atoms_uuid] = calcjob_nscf
            print(f"NSCF calcjob for structure {atoms_uuid} submitted successfully with UUID: {calcjob_nscf.uuid}")

    dict_folderdata_nodes = {}
    for atoms_uuid, calcjob_nscf in uuids_nscf_calcjob_dict.items():
        import time
        if calcjob_nscf is not None:
            want_nscf_finished = False
            while not want_nscf_finished:
                if calcjob_nscf.is_terminated:
                    print(f"Calcjob {calcjob_nscf.uuid} is already terminated.")
                    # new_folders = copy_calcjob_outputs_to_local(calcjob_nscf.pk)
                    t1 = time.time()
                    target_folderdata_node = copy_calcjob_outputs_to_local(calcjob_node=calcjob_nscf,
                                                                        #    list_files_paths=list_bandsx_inputs_file,
                                                                        copy_all=True)
                    dict_folderdata_nodes['atoms_uuid'] = target_folderdata_node
                    want_nscf_finished = True
                else:
                    time.sleep(3) ## wait 3s before check again
                


# %%
    for atoms_uuid, target_folderdata_node in dict_folderdata_nodes.items():
        ##### bands analysis after copying finished
        bandsx_code_name = 519429 # bands@localhost_direct
        parent_folder = target_folderdata_node ## copied from ISDM but storage as RemoteData, see above
        Para_bands = bands_calc_input()
        builder_bandsx = bands_calc_builder(code_name=bandsx_code_name,
                        parent_folder= parent_folder, ## copied GeTe calcjob's output folder from cluster and store as a FolderData node519468
                        inputs=Para_bands
                        )
        calcjob_bandsx = submit(builder_bandsx)
    


# %% debug for some materials
    atoms_uuid = 'de5d09ce-bf2d-4ba9-87d7-f342b2a2a636' # As203 P6mm
    input_para = config_input_para_pw(scf=True)
    code_name = 519349 # pw@ISDM_jy
    calcjob_scf, calcjob_nscf = workflow_scf_nscf(atoms_uuid, 
                                                code_name, 
                                                input_para, 
                                                pseudo_family_name='PseudoDojo/0.4/PBE/FR/standard/upf',
                                                k_points_mesh = [10, 10, 1],
                                                k_points_mesh_nscf = [24, 24, 1],
                                                k_points_offset = [0, 0, 0],
                                                n_cond_bands = 6,
                                                withmpi = True,
                                                num_node = 1,
                                                queue_name = 'cpu-ondemand',
                                                qos ='ondemand-short',
                                                time = '00:30:00',
                                                prepend_text = 'module load bioinfo-ifb openmpi/4.0.4 gcc/11.2.0\nexport NEWVAR=1'
                                                
                                                )
# %% graph
    g = Graph()
    results_uuid = ['de5d09ce-bf2d-4ba9-87d7-f342b2a2a636', ## As2O3 P6mm C6v
                    '9be800e5-41ac-4875-8746-e10b6995cb8b', ## AsSb P3m1 C3v
                    # '481b517b-3fd8-4fad-8608-ce2846c61ed1', ##  PbTe P3m1 C3v
                    ]
    for uuid in results_uuid:
        g.add_node(load_node(uuid))
        g.recurse_descendants(load_node(uuid),depth=3)
        # g.add_node(calcjob_nscf)
    
    g.graphviz


# %% copied manually remote folder of known nscf calcjob to local computer for bands.x calculation test
    calcjob_pks = {
        'As2O3' : 519565,
        'AsSb' : 519579
    }
    for material, pk in calcjob_pks.items():
        calcjob_node = load_node(pk)
        target_folderdata_node = copy_calcjob_outputs_to_local(calcjob_node=calcjob_node,
                                                            #    list_files_paths=list_bandsx_inputs_file,
                                                               copy_all=True)
        ##### bands analysis after copying finished
        # bandsx_code_name = 519429
# %% find local copied nodes by calcjob uuid
    local_nodes = {}
    for material, pk in calcjob_pks.items():
        calcjob_node = load_node(pk)
        uuid_calcjob = str(calcjob_node.uuid)
        found_node = find_nodes_by_calcjob_uuid(uuid_calcjob)
        # found_node = load_node(found_node)
        print(f"Found nodes for calcjob UUID {uuid_calcjob}: {found_node}")
        local_nodes[material] = found_node
# %% prepare bands.x calcjob builder
    for material, node in local_nodes.items():
        
        parent_folder = node
        bandsx_code_name = 519429 # bands@localhost_direct
        Para_bands = bands_calc_input()
        builder_bandsx = bands_calc_builder(code_name=bandsx_code_name,
                        parent_folder= parent_folder, ## copied GeTe calcjob's output folder from cluster and store as a FolderData node519468
                        inputs=Para_bands
                        )
        calcjob_bandsx = submit(builder_bandsx)
